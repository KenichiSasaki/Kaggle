{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [U+306F, 1231, 3465, 133, 53, U+304C, 275, 165...\n",
       "1    [U+306F, 1087, 2018, 103, 65, U+304B, 1456, 18...\n",
       "2    [U+306F, 572, 1376, 125, 57, U+306E, 1551, 208...\n",
       "Name: labels, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # -*- coding: utf-8 -*-\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import japanize_matplotlib\n",
    "import os\n",
    "\n",
    "train_file = pd.read_csv(r\"C:\\Users\\Kenichi\\Documents\\Kaggle_Data\\kuzushiji_recognition\\train.csv\")\n",
    "\n",
    "unicode_chart = pd.read_csv(r\"C:\\Users\\Kenichi\\Documents\\Kaggle_Data\\kuzushiji_recognition\\unicode_translation.csv\")\n",
    "train_file.labels = train_file.labels.str.split(' ') #(3881, 2)\n",
    "train_file.labels.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['は' 'が' 'い' 'け' 'に' 'に' 'の' '工' 'こ' '三' 'の' 'や' 'と' '己' 'も' 'さ' 'の' 'の'\n",
      " '世' '細' 'そ' '人' 'け' 'れ' 'だ' '強' 'の' 'た' 'て' '俳' '根' 'か' '諧' 'れ' '子' 'を'\n",
      " '及' '者' 'ず' '避' '正' '文' 'ふ' '時' '思' 'な' 'め' '浮' 'を' '気' '職' '老' '武' '楽'\n",
      " 'も' '盲' '若' '自' 'ど' '風' 'に' '裏' '畳' '息' '序' 'し' 'し']\n"
     ]
    }
   ],
   "source": [
    "#changed the train label dimension\n",
    "label_np = np.array(train_file.labels[0])\n",
    "label_np = label_np.reshape(int(len(label_np)/5), 5)\n",
    "label_list = np.array([])\n",
    "for i in label_np[:, 0]:\n",
    "    label_list= np.append(label_list, unicode_chart.char[unicode_chart.Unicode == i])\n",
    "#label_list = pd.DataFrame(label_list, columns=[\"chart_index\", \"char\"])\n",
    "print(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 0\n",
      "k: 100\n",
      "k: 200\n",
      "k: 300\n",
      "k: 400\n",
      "k: 500\n",
      "k: 600\n",
      "k: 700\n",
      "k: 800\n",
      "k: 1000\n"
     ]
    }
   ],
   "source": [
    "def get_padded_image(crop):\n",
    "    h, w = crop.shape\n",
    "    if w > h:\n",
    "        crop_resize = cv2.resize(crop , (64, int(h/(w/64))))\n",
    "    else:\n",
    "        ratio = int(h/64)\n",
    "        crop_resize = cv2.resize(crop , (int(w/(h/64)), 64))\n",
    "\n",
    "    n_h, n_w = crop_resize.shape\n",
    "    #print(int((64 - n_h)/2),int((64 - n_w)/2))\n",
    "    \n",
    "    # polarize\n",
    "    crop_resize[np.where(crop_resize > 180)] = 255\n",
    "    crop_resize[np.where(crop_resize <= 180)] = 0\n",
    "\n",
    "    # Padding to 64 * 64 pixels \n",
    "    bg_color = 255  # crop_resize[0, 0]\n",
    "    if (64 - n_h)%2 == 0 and (64 - n_w)%2 == 0:\n",
    "        padded_img = cv2.copyMakeBorder(crop_resize,int((64 - n_h)/2), int((64 - n_h)/2), \n",
    "                                    int((64 - n_w)/2), int((64 - n_w)/2), cv2.BORDER_CONSTANT, value = bg_color)\n",
    "    elif (64 - n_h)%2 != 0 and (64 - n_w)%2 == 0:\n",
    "        padded_img = cv2.copyMakeBorder(crop_resize,int((64 - n_h)/2), int((64 - n_h)/2) + 1, \n",
    "                                    int((64 - n_w)/2), int((64 - n_w)/2), cv2.BORDER_CONSTANT, value = bg_color)\n",
    "    elif (64 - n_h)%2 == 0 and (64 - n_w)%2 != 0:\n",
    "        padded_img = cv2.copyMakeBorder(crop_resize,int((64 - n_h)/2), int((64 - n_h)/2), \n",
    "                                    int((64 - n_w)/2), int((64 - n_w)/2) + 1, cv2.BORDER_CONSTANT, value = bg_color)        \n",
    "    return padded_img\n",
    "\n",
    "img_assem = np.empty((0, 64, 64), int)\n",
    "label_assem = np.array([])\n",
    "\n",
    "for k in range(train_file.shape[0]): #train_file.shape[0]\n",
    "    label_np = np.array(train_file.labels[k])\n",
    "    if pd.isnull(label_np).all() == True:\n",
    "        continue\n",
    "    label_np = label_np.reshape(int(len(label_np)/5), 5)\n",
    "    label_list = np.array([])\n",
    "    for i in label_np[:, 0]:\n",
    "        label_list= np.append(label_list, unicode_chart.char[unicode_chart.Unicode == i])\n",
    "    \n",
    "    image = cv2.imread(os.path.join(r\"C:\\Users\\Kenichi\\Documents\\Kaggle_Data\\kuzushiji_recognition\\train_images\", \n",
    "                                    train_file.image_id[k] + \".jpg\"))\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)   \n",
    "    height, width = img.shape\n",
    "    \n",
    "#     fig =plt.figure(figsize=(width/200, height/200))\n",
    "#     ax = fig.add_subplot(1,1,1)\n",
    "    j = 0\n",
    "    \n",
    "    for i in label_np:\n",
    "        rect = mpatches.Rectangle((int(i[1]), int(i[2])), int(i[3]), int(i[4]), fill=False, edgecolor='red', linewidth=1)\n",
    "#         ax.add_patch(rect)\n",
    "        crop = img[ int(i[2]):int(i[2]) + int(i[4]), int(i[1]):int(i[1]) + int(i[3])]\n",
    "        crop = get_padded_image(crop)\n",
    "#         print(crop.shape)\n",
    "#         cv2.imwrite(os.path.join(\"../../Kaggle_Data/kuzushiji_recognition/cropped\", train_file.image_id[k] + \n",
    "#                                  \"_\" + str(i[0]) + \"_\" + str(i[1]) + \"_\" + str(i[2]) + \"_\"\n",
    "#                                  + str(i[3]) + \"_\" + str(i[4]) + \".jpg\"), crop)\n",
    "        img_assem = np.append(img_assem, np.array([crop]), axis = 0)\n",
    "        label_assem = np.append(label_assem, str(i[0]))\n",
    "#         ax.text((int(i[1]) + int(i[3]))/width, 1 - int(i[2])/height, label_list[j],\n",
    "#                 horizontalalignment='left',fontsize=20,\n",
    "#                 verticalalignment='center',\n",
    "#                 rotation='horizontal',\n",
    "#                 transform=ax.transAxes)\n",
    "        j = j+1\n",
    "#     ax.imshow(img)\n",
    "#     plt.savefig(os.path.join(\"../../Kaggle_Data/kuzushiji_recognition/anotated\", train_file.image_id[k]+\"_anotated.jpg\"))\n",
    "#plt.show()\n",
    "    if k%100 == 0:\n",
    "        print(\"k:\", k)\n",
    "        np.save('../../Kaggle_Data\\kuzushiji_recognition\\image_np', img_assem)\n",
    "        np.save('../../Kaggle_Data\\kuzushiji_recognition\\label_np', label_assem)\n",
    "#     if k%500 == 0:\n",
    "#         img_assem = np.empty((0, 64, 64), int)\n",
    "#         label_assem = np.array([])\n",
    "\n",
    "print(img_assem.shape)\n",
    "print(label_assem.shape)\n",
    "np.save('../../Kaggle_Data\\kuzushiji_recognition\\image_np', img_assem)\n",
    "np.save('../../Kaggle_Data\\kuzushiji_recognition\\label_np', label_assem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15\n"
     ]
    }
   ],
   "source": [
    "def(crop):\n",
    "    h, w, ch = crop.shape\n",
    "    if w > h:\n",
    "        crop_resize = cv2.resize(crop , (64, int(h/(w/64))))\n",
    "    else:\n",
    "        ratio = int(h/64)\n",
    "        crop_resize = cv2.resize(crop , (int(w/(h/64)), 64))\n",
    "\n",
    "    n_h, n_w, ch = crop_resize.shape\n",
    "    print(int((64 - n_h)/2),int((64 - n_w)/2))\n",
    "    bg_color = [198, 244, 252]\n",
    "    if int((64 - n_h)/2)%2 == 0 and int((64 - n_w)/2)%2 == 0:\n",
    "        padded_img = cv2.copyMakeBorder(crop_resize,int((64 - n_h)/2), int((64 - n_h)/2), \n",
    "                                    int((64 - n_w)/2), int((64 - n_w)/2), cv2.BORDER_CONSTANT, value = bg_color)\n",
    "    elif: int((64 - n_h)/2)%2 != 0 and int((64 - n_w)/2)%2 == 0:\n",
    "        padded_img = cv2.copyMakeBorder(crop_resize,int((64 - n_h)/2), int((64 - n_h)/2) + 1, \n",
    "                                    int((64 - n_w)/2), int((64 - n_w)/2), cv2.BORDER_CONSTANT, value = bg_color)\n",
    "    elif: int((64 - n_h)/2)%2 == 0 and int((64 - n_w)/2)%2 != 0:\n",
    "        padded_img = cv2.copyMakeBorder(crop_resize,int((64 - n_h)/2), int((64 - n_h)/2) + 1, \n",
    "                                    int((64 - n_w)/2), int((64 - n_w)/2) + 1, cv2.BORDER_CONSTANT, value = bg_color)\n",
    "            \n",
    "    return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128    ゝ\n",
       "Name: char, dtype: object"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_chart.char[unicode_chart.Unicode == \"U+309D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['U+306F', 'U+304C', 'U+3044', 'U+3051', 'U+306B', 'U+306B',\n",
       "       'U+306E', 'U+5DE5', 'U+3053', 'U+4E09', 'U+306E', 'U+3084',\n",
       "       'U+3068', 'U+5DF1', 'U+3082', 'U+3055', 'U+306E', 'U+306E',\n",
       "       'U+4E16', 'U+7D30', 'U+305D', 'U+4EBA', 'U+3051', 'U+308C',\n",
       "       'U+3060', 'U+5F37', 'U+306E', 'U+305F', 'U+3066', 'U+4FF3',\n",
       "       'U+6839', 'U+304B', 'U+8AE7', 'U+308C', 'U+5B50', 'U+3092',\n",
       "       'U+53CA', 'U+8005', 'U+305A', 'U+907F', 'U+6B63', 'U+6587',\n",
       "       'U+3075', 'U+6642', 'U+601D', 'U+306A', 'U+3081', 'U+6D6E',\n",
       "       'U+3092', 'U+6C17', 'U+8077', 'U+8001', 'U+6B66', 'U+697D',\n",
       "       'U+3082', 'U+76F2', 'U+82E5', 'U+81EA', 'U+3069', 'U+98A8',\n",
       "       'U+306B', 'U+88CF', 'U+7573', 'U+606F', 'U+5E8F', 'U+3057',\n",
       "       'U+3057'], dtype='<U32')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('../../Kaggle_Data\\kuzushiji_recognition\\label_np.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "2.2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kenichi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Kenichi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 192)         221376    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 192)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 256)         442624    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,851,658\n",
      "Trainable params: 2,850,378\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Kenichi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kenichi\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/74\n",
      " 6144/50000 [==>...........................] - ETA: 6:00 - loss: 2.5207 - acc: 0.2005"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-c318f7a2f402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=74, verbose=1,\n\u001b[1;32m---> 73\u001b[1;33m               callbacks=[rlop], validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Lambda, Input, Dense, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dropout\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau,TensorBoard\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.datasets import cifar10\n",
    "import cv2\n",
    "# import gc\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "# Prepare data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "enc = OneHotEncoder()\n",
    "y_train = enc.fit_transform(y_train).toarray()\n",
    "y_test = enc.fit_transform(y_test).toarray()\n",
    "\n",
    "inputs = Input(shape=(32, 32,3))\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block1_pool')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block2_pool')(x)\n",
    "x = Conv2D(192, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block3_pool')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block5_pool')(x)\n",
    "flattened = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc1')(flattened)\n",
    "x = Dropout(0.5, name='dropout1')(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = Dropout(0.5, name='dropout2')(x)\n",
    "predictions = Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "sgd = optimizers.SGD(lr=0.01,\n",
    "                     momentum=0.9,\n",
    "                     decay=5e-4)#, nesterov=False)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "rlop = ReduceLROnPlateau(monitor='val_acc',\n",
    "                         factor=0.1,\n",
    "                         patience=5,\n",
    "                         verbose=1,\n",
    "                         mode='auto',\n",
    "                         epsilon=0.0001,\n",
    "                         cooldown=0,\n",
    "                         min_lr=0.00001)\n",
    "\n",
    " \n",
    "model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=74, verbose=1,\n",
    "              callbacks=[rlop], validation_data=(x_test, y_test))\n",
    "\n",
    "y_pred = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
